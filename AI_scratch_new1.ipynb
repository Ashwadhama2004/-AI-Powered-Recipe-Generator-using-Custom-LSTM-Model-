{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         ingredients  \\\n",
      "0  1 12 pound flank steak, 12 c finely minced gre...   \n",
      "1  1 tablespoon rosemary, 1 teaspoon thyme, 3 bay...   \n",
      "2  3 to 4 carrots, 1 12 tbsp butter, 13 c brown s...   \n",
      "3  45 cups flour, 15 tsp salt, pinch baking powde...   \n",
      "4  2 c crushed small thin pretzels sticks, 34 c m...   \n",
      "\n",
      "                                          directions  \\\n",
      "0  remove tenderloin from steak, score meat, comb...   \n",
      "1  combine all ingredients in slow cooker 6 quart...   \n",
      "2  cook 3 to 4 carrots cut crosswise in 1inch pie...   \n",
      "3  mix all dry ingredients in a bowl, , add crisc...   \n",
      "4  mix and press in baking pan, approximately 13 ...   \n",
      "\n",
      "                                          input_text  \\\n",
      "0  Ingredients: 1 12 pound flank steak, 12 c fine...   \n",
      "1  Ingredients: 1 tablespoon rosemary, 1 teaspoon...   \n",
      "2  Ingredients: 3 to 4 carrots, 1 12 tbsp butter,...   \n",
      "3  Ingredients: 45 cups flour, 15 tsp salt, pinch...   \n",
      "4  Ingredients: 2 c crushed small thin pretzels s...   \n",
      "\n",
      "                                         target_text  \n",
      "0  Directions: remove tenderloin from steak, scor...  \n",
      "1  Directions: combine all ingredients in slow co...  \n",
      "2  Directions: cook 3 to 4 carrots cut crosswise ...  \n",
      "3  Directions: mix all dry ingredients in a bowl,...  \n",
      "4  Directions: mix and press in baking pan, appro...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv(\"RecipeNLG_dataset.csv\")[['ingredients', 'directions']].dropna()\n",
    "\n",
    "# Keep only 100,000 samples\n",
    "dataset = dataset.sample(n=50000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "dataset[\"ingredients\"] = dataset[\"ingredients\"].str.lower()\n",
    "dataset[\"directions\"] = dataset[\"directions\"].str.lower()\n",
    "dataset[\"ingredients\"] = dataset[\"ingredients\"].apply(lambda x: re.sub(r\"[^a-zA-Z0-9, ]\", \"\", x))\n",
    "dataset[\"directions\"] = dataset[\"directions\"].apply(lambda x: re.sub(r\"[^a-zA-Z0-9, ]\", \"\", x))\n",
    "\n",
    "dataset[\"input_text\"] = \"Ingredients: \" + dataset[\"ingredients\"]\n",
    "dataset[\"target_text\"] = \"Directions: \" + dataset[\"directions\"]\n",
    "\n",
    "# Display the first few rows\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 40000, Validation size: 5000, Test size: 5000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-validation-test split (80% train, 10% val, 10% test)\n",
    "train_data, temp_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=64):\n",
    "        self.input_texts = data[\"input_text\"].tolist()\n",
    "        self.target_texts = data[\"target_text\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_enc = self.tokenizer(\n",
    "            self.input_texts[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_enc = self.tokenizer(\n",
    "            self.target_texts[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target_enc[\"input_ids\"].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input: tensor([23482,     7,    10,   220,  1669,   158, 14693,     6,   158,   400])\n",
      "Sample Target: tensor([19436,     7,    10, 14514,     8,   158,  1836, 18647,   859,   662])\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = RecipeDataset(train_data, tokenizer)\n",
    "val_dataset = RecipeDataset(val_data, tokenizer)\n",
    "test_dataset = RecipeDataset(test_data, tokenizer)\n",
    "\n",
    "# Print sample input-output\n",
    "print(\"Sample Input:\", train_dataset[0][\"input_ids\"][:10])  # First 10 tokens of input\n",
    "print(\"Sample Target:\", train_dataset[0][\"labels\"][:10])    # First 10 tokens of target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 2500, Validation batches: 313, Test batches: 313\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print batch sizes to confirm\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\conda\\envs\\trainy2_O\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cpu\n",
      "✅ Model, Optimizer, Scheduler, and Data Loaders initialized!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader  # Replace with actual dataset\n",
    "\n",
    "# ✅ Check for CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ✅ Load T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# ✅ Set dropout rates\n",
    "model.config.dropout_rate = 0.3  # Default is 0.1, increased for better regularization\n",
    "model.config.attention_dropout = 0.3\n",
    "\n",
    "# ✅ Move model to CPU\n",
    "model.to(device)\n",
    "\n",
    "# ✅ Define DataLoader (Replace with actual dataset)\n",
    "train_loader = DataLoader([])  # Replace with actual dataset\n",
    "val_loader = DataLoader([])    # Replace with actual dataset\n",
    "\n",
    "# ✅ Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# ✅ Define learning rate scheduler\n",
    "num_epochs = 3\n",
    "num_training_steps = max(len(train_loader) * num_epochs, 1)  # Prevents division by zero\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "print(\"✅ Model, Optimizer, Scheduler, and Data Loaders initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "✅ Model, Optimizer, and Scheduler initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0it [00:00, ?it/s]\n",
      "Validating Epoch 1: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.0000 | Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: 0it [00:00, ?it/s]\n",
      "Validating Epoch 2: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.0000 | Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 0it [00:00, ?it/s]\n",
      "Validating Epoch 3: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.0000 | Validation Loss: 0.0000\n",
      "✅ Training Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ✅ Check for CPU (No GPU usage)\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ✅ Load T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# ✅ Set dropout rates\n",
    "model.config.dropout_rate = 0.3  # Default is 0.1, increase if needed\n",
    "model.config.attention_dropout = 0.3\n",
    "\n",
    "# ✅ Move model to CPU\n",
    "model.to(device)\n",
    "\n",
    "# ✅ Define dummy train_loader (Replace with actual data)\n",
    "train_loader = DataLoader([])  # Replace with your dataset\n",
    "val_loader = DataLoader([])  # Replace with actual validation data\n",
    "\n",
    "# ✅ Define optimizer (using torch's AdamW)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# ✅ Define learning rate scheduler\n",
    "num_epochs = 3\n",
    "num_training_steps = max(len(train_loader) * num_epochs, 1)  # Prevents division by zero\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "print(\"✅ Model, Optimizer, and Scheduler initialized!\")\n",
    "\n",
    "# === Training Loop ===\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", total=len(train_loader), leave=True)\n",
    "\n",
    "    for batch in loop:\n",
    "        batch = {k: v.to(device, dtype=torch.long) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # ✅ Gradient Clipping (Prevents Exploding Gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # ✅ Show loss in tqdm\n",
    "        loop.set_postfix(train_loss=loss.item())\n",
    "\n",
    "    # ✅ Move scheduler step **outside** batch loop (once per epoch)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # === Validation Loop ===\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loop = tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}\", total=len(val_loader), leave=True)\n",
    "        for batch in val_loop:\n",
    "            batch = {k: v.to(device, dtype=torch.long) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss.item()\n",
    "            total_val_loss += loss\n",
    "\n",
    "            val_loop.set_postfix(val_loss=loss)\n",
    "\n",
    "    # === Logging Loss ===\n",
    "    avg_train_loss = total_train_loss / max(len(train_loader), 1)  # Avoid division by zero\n",
    "    avg_val_loss = total_val_loss / max(len(val_loader), 1)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # ✅ Save Best Model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_t5_model.pth\")\n",
    "\n",
    "print(\"✅ Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 7.5454\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            total_test_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Evaluate model on test set\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Ingredients: 1 c selfrising flour, 1 c plain flour, 1 teaspoon baking soda, 2 large eggs, 12 stick butter, 2 2 onions, minced, 4 tbsp sugar, healthy pinch of salt, 1 stick celery, minced, add in beer till thick\n",
      "Generated Recipe: : 1 c selfrising flour, 1 c plain flour, 1 teaspoon baking soda, 2 large eggs, 12 stick butter, 2 2 onions, minced, 4 tbsp sugar, healthy pinch of salt, 1 stick celery, minced, add in beer till thick, 1 c selfrising flour, 1 c plain flour, 1 teaspoon baking soda, 2 large eggs, 12 stick butter, 2 2 onions, minced, 4 tbsp sugar, healthy pinch of salt, 1 stick cele\n"
     ]
    }
   ],
   "source": [
    "# Recipe generation function\n",
    "def generate_recipe(ingredients):\n",
    "    model.eval()\n",
    "    input_text = \"Ingredients: \" + ingredients\n",
    "    input_enc = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**input_enc, max_length=150)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the recipe generation function\n",
    "sample_ingredients = test_data.iloc[0][\"ingredients\"]\n",
    "generated_recipe = generate_recipe(sample_ingredients)\n",
    "\n",
    "print(f\"Sample Ingredients: {sample_ingredients}\")\n",
    "print(f\"Generated Recipe: {generated_recipe}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainy2_O",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
